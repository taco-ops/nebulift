# NFS-based training job for distributed FITS data processing
# This version uses NFS storage for shared data access across all pods

apiVersion: batch/v1
kind: Job
metadata:
  name: nebulift-training-nfs
  labels:
    app: nebulift-training
spec:
  parallelism: 4  # Number of training pods (adjust for RPi5 cluster size)
  completions: 4
  completionMode: Indexed  # Enable indexed job for proper rank assignment
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: nebulift-training
    spec:
      restartPolicy: Never

      # Use node affinity to distribute across RPi5 nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - nebulift-training
                topologyKey: kubernetes.io/hostname

      containers:
        - name: nebulift-trainer
          image: ghcr.io/taco-ops/nebulift:latest  # Use latest published image
          imagePullPolicy: IfNotPresent

          command: ["python", "-m", "nebulift.distributed.k8s_trainer"]

          env:
            # Distributed training environment
            - name: WORLD_SIZE
              value: "4"  # Total number of processes
            - name: RANK
              valueFrom:
                fieldRef:
                  fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
            - name: MASTER_ADDR
              value: "nebulift-training-headless.nebulift.svc.cluster.local"
            - name: MASTER_PORT
              value: "29500"

            # Training configuration from ConfigMap
            - name: LEARNING_RATE
              valueFrom:
                configMapKeyRef:
                  name: nebulift-training-config
                  key: learning_rate
            - name: BATCH_SIZE
              valueFrom:
                configMapKeyRef:
                  name: nebulift-training-config
                  key: batch_size
            - name: EPOCHS
              valueFrom:
                configMapKeyRef:
                  name: nebulift-training-config
                  key: epochs
            - name: TRAIN_DATA_PATH
              valueFrom:
                configMapKeyRef:
                  name: nebulift-training-config
                  key: train_data_path
            - name: VAL_DATA_PATH
              valueFrom:
                configMapKeyRef:
                  name: nebulift-training-config
                  key: val_data_path
            - name: MODEL_OUTPUT_PATH
              valueFrom:
                configMapKeyRef:
                  name: nebulift-training-config
                  key: model_output_path

          resources:
            requests:
              memory: "1Gi"
              cpu: "1"
            limits:
              memory: "2Gi"
              cpu: "2"

          volumeMounts:
            - name: nfs-data
              mountPath: /data
              readOnly: true
            - name: nfs-models
              mountPath: /models
            - name: nfs-coordination
              mountPath: /shared
            - name: local-cache
              mountPath: /tmp/cache

      volumes:
        # Use proper NFS volumes that can be accessed from any node
        - name: nfs-data
          nfs:
            server: 10.42.0.22
            path: /exports/cluster-storage/nebulift/data
        - name: nfs-models
          nfs:
            server: 10.42.0.22
            path: /exports/cluster-storage/nebulift/models
        - name: nfs-coordination
          nfs:
            server: 10.42.0.22
            path: /exports/cluster-storage
        - name: local-cache
          emptyDir:
            sizeLimit: 10Gi  # Per-pod cache

      # Remove node selector to allow distribution across all nodes
      # nodeSelector:
      #   node-role.kubernetes.io/control-plane: "true"

      # Tolerations for all node types
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/arch"
          operator: "Equal"
          value: "arm64"
          effect: "NoSchedule"
        - key: "kubernetes.io/arch"
          operator: "Equal"
          value: "arm64"
          effect: "NoSchedule"
