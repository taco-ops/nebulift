name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  UV_CACHE_DIR: /tmp/.uv-cache

jobs:
  lint-and-format:
    name: Lint and Format Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Restore uv cache
      uses: actions/cache@v4
      with:
        path: /tmp/.uv-cache
        key: uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
        restore-keys: |
          uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
          uv-${{ runner.os }}

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Check code formatting with Black
      run: uv run black --check --diff .

    - name: Lint with flake8
      run: uv run flake8 nebulift/ tests/

    - name: Type check with mypy
      run: uv run mypy nebulift/ --ignore-missing-imports

    - name: Minimize uv cache
      run: uv cache prune --ci

  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ["3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python ${{ matrix.python-version }}
      run: uv python install ${{ matrix.python-version }}

    - name: Restore uv cache
      uses: actions/cache@v4
      with:
        path: /tmp/.uv-cache
        key: uv-${{ matrix.os }}-${{ matrix.python-version }}-${{ hashFiles('uv.lock') }}
        restore-keys: |
          uv-${{ matrix.os }}-${{ matrix.python-version }}-${{ hashFiles('uv.lock') }}
          uv-${{ matrix.os }}-${{ matrix.python-version }}
          uv-${{ matrix.os }}

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Run tests with coverage
      run: |
        uv run pytest tests/ -v --cov=nebulift --cov-report=xml --cov-report=term-missing

    - name: Run system validation
      run: uv run python validate_system.py

    - name: Test model persistence
      run: uv run python test_model_persistence.py

    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Minimize uv cache
      run: uv cache prune --ci

  test-distributed:
    name: Test Distributed Training
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Test distributed training components
      run: |
        uv run pytest tests/distributed/ -v -k "not kubernetes" || echo "Distributed tests completed"

    - name: Test data sharding
      run: |
        uv run python -c "
        from nebulift.distributed.data_sharding import create_distributed_dataloaders
        from nebulift.ml_model import AstroImageDataset
        from pathlib import Path
        import tempfile
        import torch

        # Create mock dataset
        with tempfile.TemporaryDirectory() as temp_dir:
            images = [Path(temp_dir) / f'image_{i}.jpg' for i in range(10)]
            labels = [0, 1] * 5
            dataset = AstroImageDataset(images, labels)

            # Test sharding
            train_loader, val_loader = create_distributed_dataloaders(
                dataset, dataset, world_size=2, rank=0, batch_size=2
            )
            print('âœ“ Data sharding test passed')
        "

  test-container:
    name: Test Container Build
    runs-on: ubuntu-latest
    needs: [lint-and-format, test]
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build and test container (AMD64 only for testing)
      run: |
        docker build --platform linux/amd64 -t nebulift:test .
        docker run --rm nebulift:test python -c "import nebulift; print('âœ“ Container import test passed')"

  build-amd64:
    name: Build AMD64 Container
    runs-on: ubuntu-latest
    needs: [test-container]
    if: github.ref == 'refs/heads/main'
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}

    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to GitHub Container Registry
      if: github.event_name != 'pull_request'
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      if: github.event_name != 'pull_request'
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ghcr.io/${{ github.repository }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push AMD64 container image
      if: github.event_name != 'pull_request'
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64
        push: true
        tags: ${{ steps.meta.outputs.tags }}-amd64
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha,key=amd64
        cache-to: type=gha,mode=max,key=amd64
        provenance: false

  build-arm64:
    name: Build ARM64 Container
    runs-on: ubuntu-latest
    needs: [test-container]
    if: github.ref == 'refs/heads/main'
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}

    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to GitHub Container Registry
      if: github.event_name != 'pull_request'
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      if: github.event_name != 'pull_request'
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ghcr.io/${{ github.repository }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push ARM64 container image
      if: github.event_name != 'pull_request'
      id: build
      uses: docker/build-push-action@v5
      timeout-minutes: 45
      continue-on-error: true
      with:
        context: .
        platforms: linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}-arm64
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha,key=arm64
        cache-to: type=gha,mode=max,key=arm64
        provenance: false

  create-manifest:
    name: Create Multi-Platform Manifest
    runs-on: ubuntu-latest
    needs: [build-amd64, build-arm64]
    if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request' && always()

    steps:
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to GitHub Container Registry
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ghcr.io/${{ github.repository }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Create and push multi-architecture manifest
      run: |
        echo "AMD64 build result: ${{ needs.build-amd64.result }}"
        echo "ARM64 build result: ${{ needs.build-arm64.result }}"

        # Enable Docker experimental features for manifest commands
        export DOCKER_CLI_EXPERIMENTAL=enabled

        # Parse tags from metadata step
        TAGS="${{ steps.meta.outputs.tags }}"

        if [[ "${{ needs.build-amd64.result }}" == "success" ]]; then
          echo "âœ… AMD64 build successful"
          AMD64_SUCCESS=true
        else
          echo "âŒ AMD64 build failed"
          AMD64_SUCCESS=false
          exit 1  # AMD64 is required
        fi

        if [[ "${{ needs.build-arm64.result }}" == "success" ]]; then
          echo "âœ… ARM64 build successful"
          ARM64_SUCCESS=true
        else
          echo "âš ï¸ ARM64 build failed or skipped"
          ARM64_SUCCESS=false
        fi

        # Create manifest for each tag
        for tag in $TAGS; do
          echo "Creating manifest for tag: $tag"

          # Check if platform-specific images exist
          echo "Checking for platform images:"
          echo "  AMD64: $tag-amd64"
          echo "  ARM64: $tag-arm64"

          if [[ "$ARM64_SUCCESS" == "true" ]]; then
            # Multi-architecture manifest
            echo "Creating multi-architecture manifest"

            # Remove existing manifest if it exists
            docker manifest rm "$tag" 2>/dev/null || true

            # Create new manifest
            docker manifest create "$tag" \
              "$tag-amd64" \
              "$tag-arm64"

            # Inspect the created manifest
            docker manifest inspect "$tag"

            # Push the manifest
            docker manifest push "$tag"

            echo "ðŸŽ‰ Multi-platform image created: $tag (amd64 + arm64)"
          else
            # AMD64 only manifest
            echo "Creating AMD64-only manifest"

            # Remove existing manifest if it exists
            docker manifest rm "$tag" 2>/dev/null || true

            # Create new manifest with just AMD64
            docker manifest create "$tag" "$tag-amd64"

            # Inspect the created manifest
            docker manifest inspect "$tag"

            # Push the manifest
            docker manifest push "$tag"

            echo "âœ… Single-platform image created: $tag (amd64 only)"
          fi
        done  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Run safety check for vulnerabilities
      run: |
        uv run pip install safety
        uv run safety check || echo "Safety scan completed with warnings"

    - name: Run bandit security linter
      run: |
        uv run pip install bandit
        uv run bandit -r nebulift/ -f json -o bandit-report.json || echo "Bandit scan completed"
        uv run bandit -r nebulift/ || echo "Bandit scan completed with warnings"

    - name: Upload bandit results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: bandit-results
        path: bandit-report.json

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Run performance benchmarks
      run: |
        uv run python -c "
        import time
        import tempfile
        from pathlib import Path
        from nebulift.fits_processor import FITSProcessor
        from nebulift.cv_prefilter import ArtifactDetector
        from nebulift.ml_model import AstroQualityClassifier, QualityPredictor
        import numpy as np

        print('ðŸ”¥ Running performance benchmarks...')

        # FITS processing benchmark
        processor = FITSProcessor()
        start_time = time.time()

        # Create mock FITS data
        mock_data = np.random.random((1024, 1024)).astype(np.float32)
        normalized = processor.normalize_image(mock_data)

        fits_time = time.time() - start_time
        print(f'âœ“ FITS processing: {fits_time:.3f}s for 1024x1024 image')

        # CV analysis benchmark
        detector = ArtifactDetector()
        start_time = time.time()

        analysis = detector.comprehensive_analysis(normalized)

        cv_time = time.time() - start_time
        print(f'âœ“ CV analysis: {cv_time:.3f}s')

        # ML inference benchmark (CPU-only)
        model = AstroQualityClassifier(pretrained=False)
        model.eval()

        # Prepare input
        ml_input = processor.resize_for_ml(normalized)

        start_time = time.time()
        with tempfile.NamedTemporaryFile(suffix='.pth') as f:
            import torch
            torch.save({'model_state_dict': model.state_dict(),
                       'model_class': 'AstroQualityClassifier',
                       'model_config': {'num_classes': 2, 'dropout_rate': 0.2}}, f.name)

            predictor = QualityPredictor(f.name, device='cpu')

        load_time = time.time() - start_time
        print(f'âœ“ Model loading: {load_time:.3f}s')

        print(f'ðŸ“Š Total pipeline benchmark: {fits_time + cv_time + load_time:.3f}s')
        print('ðŸŽ¯ Performance benchmark completed successfully!')
        "

  release:
    name: Create Release
    runs-on: ubuntu-latest
    needs: [lint-and-format, test, test-distributed, create-manifest, security-scan]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Generate changelog
      id: changelog
      run: |
        echo "CHANGELOG<<EOF" >> $GITHUB_OUTPUT
        echo "## Changes" >> $GITHUB_OUTPUT
        git log --pretty=format:"- %s" $(git describe --tags --abbrev=0 2>/dev/null || git rev-list --max-parents=0 HEAD)..HEAD >> $GITHUB_OUTPUT
        echo "" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT

    - name: Get version from pyproject.toml
      id: version
      run: |
        VERSION=$(uv run python -c "import tomllib; print(tomllib.load(open('pyproject.toml', 'rb'))['project']['version'])")
        echo "version=$VERSION" >> $GITHUB_OUTPUT

    - name: Create Release
      if: steps.version.outputs.version != ''
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: v${{ steps.version.outputs.version }}
        release_name: Release v${{ steps.version.outputs.version }}
        body: |
          # Nebulift v${{ steps.version.outputs.version }}

          Astrophotography Quality Assessment with ResNet18 and Distributed Training

          ${{ steps.changelog.outputs.CHANGELOG }}

          ## Container Images
          - `ghcr.io/${{ github.repository }}:latest`
          - `ghcr.io/${{ github.repository }}:v${{ steps.version.outputs.version }}`

          ## Installation
          ```bash
          pip install uv
          uv sync
          ```

          ## Quick Start
          ```bash
          uv run python -m nebulift analyze path/to/fits/files/
          ```
        draft: false
        prerelease: false
